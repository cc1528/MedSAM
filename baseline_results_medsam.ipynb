{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed41ddb1edc0a0a",
   "metadata": {},
   "source": [
    "### BASELINE RESULTS MEDSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9617366860b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T20:33:03.352851Z",
     "start_time": "2024-05-29T20:33:01.509363Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%timeit import skimage.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84bb8ad52864db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T20:33:06.021204Z",
     "start_time": "2024-05-29T20:33:05.977147Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/bowang-lab/MedSAM.git\n",
    "!git clone https://github.com/bowang-lab/MedSAM\n",
    "# %% environment and functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "join = os.path.join\n",
    "import torch\n",
    "import skimage\n",
    "import skimage.io\n",
    "from skimage import transform\n",
    "import sys\n",
    "from segment_anything import sam_model_registry\n",
    "from tabulate import tabulate\n",
    "import torch.nn.functional as F\n",
    "#from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage import transform\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bdbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from segment_anything import sam_model_registry\n",
    "from MedSAM.utils.demo import BboxPromptDemo\n",
    "import os\n",
    "\n",
    "# Define the checkpoint path\n",
    "checkpoint_folder = '/kaggle/input/medsam/dataset_complete/MedSAM/medsam_vit_b'\n",
    "checkpoint_file = 'medsam_vit_b.pth'\n",
    "MedSAM_CKPT_PATH = os.path.join(checkpoint_folder, checkpoint_file)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7ca661fee7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251/255, 252/255, 30/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def medsam_inference(medsam_model, img_embed, box_1024, H, W):\n",
    "    if box_1024 is not None:\n",
    "        box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n",
    "        if len(box_torch.shape) == 2:\n",
    "            box_torch = box_torch[:, None, :]  # (B, 1, 4)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=box_torch,\n",
    "            masks=None,\n",
    "        )\n",
    "    else:\n",
    "        sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "        \n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed,  # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, gt.shape)\n",
    "    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (H, W)\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ee72adf9ba9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf7f89ba234f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define input and mask folders for the test set\n",
    "input_folder = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_2\\partitioned_dataset_original\\images\\test'\n",
    "mask_folder = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_2\\partitioned_dataset_original\\masks\\test'\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb35b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort images and masks based on numerical index\n",
    "images = sorted(os.listdir(input_folder), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "masks = sorted(os.listdir(mask_folder), key=lambda x: int(x.split('_')[-2]))\n",
    "\n",
    "# Initialize lists to store scores\n",
    "dice_scores = []\n",
    "jaccard_scores = []\n",
    "sensitivity_scores = []\n",
    "precision_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Initialize list to store inference times\n",
    "inference_times = []\n",
    "\n",
    "# Select some images to visualize the predictions\n",
    "num_visualizations = 3\n",
    "visualized_images = []\n",
    "\n",
    "\n",
    "for idx, (image_name, mask_name) in enumerate(tqdm(zip(images, masks), total=len(images))):\n",
    "    # Load images\n",
    "    image_path = os.path.join(input_folder, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Load ground truth data\n",
    "    mask_path = os.path.join(mask_folder, mask_name)\n",
    "    ground_truth = cv2.imread(mask_path)\n",
    "\n",
    "    # Convert the mask to grayscale\n",
    "    ground_truth_gray = cv2.cvtColor(ground_truth, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the grayscale mask to separate areas of interest (ROI) from the background\n",
    "    _, mask = cv2.threshold(ground_truth_gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Convert image to 1024x1024 and normalize\n",
    "    if len(image.shape) == 2:\n",
    "        img_3c = np.repeat(image[:, :, None], 3, axis=-1)  # convert to RGB\n",
    "    else:\n",
    "        img_3c = image\n",
    "    H, W, _ = img_3c.shape\n",
    "    img_1024 = transform.resize(img_3c, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)  \n",
    "    img_1024 = (img_1024 - img_1024.min()) / np.clip(\n",
    "        img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n",
    "    )  # normalize to [0, 1], (H, W, 3)\n",
    "    img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    overall_segmentation = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    if np.any(mask):\n",
    "        # Process images except for those with background label only\n",
    "        # Find contours in the mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Iterate through contours and create bounding box for each contour\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Define margin\n",
    "            margin = 10\n",
    "\n",
    "            # Expand the bounding box by the margin\n",
    "            x_margin = max(x - margin, 0)\n",
    "            y_margin = max(y - margin, 0)\n",
    "            w_margin = min(w + 2 * margin, image.shape[1] - x_margin)\n",
    "            h_margin = min(h + 2 * margin, image.shape[0] - y_margin)\n",
    "\n",
    "            # Convert the expanded bounding box coordinates to the format used in MedSAM (x_min, y_min, x_max, y_max)\n",
    "            box_np = np.array([[x_margin, y_margin, x_margin + w_margin, y_margin + h_margin]])\n",
    "\n",
    "            # Transfer box_np to 1024x1024 scale\n",
    "            box_1024 = box_np / np.array([W, H, W, H]) * 1024  # bounding box scaled to 1024x1024\n",
    "\n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Perform inference with MedSAM model\n",
    "            with torch.no_grad():\n",
    "                image_embedding = medsam_model.image_encoder(img_1024_tensor)  \n",
    "            medsam_seg = medsam_inference(medsam_model, image_embedding, box_1024, H, W)  \n",
    "\n",
    "            end_time = time.time()\n",
    "            inference_time = end_time - start_time\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            # Resize the segmentation mask back to the original image size\n",
    "            medsam_seg_resized = transform.resize(medsam_seg, (H, W), order=0, preserve_range=True).astype(np.uint8)\n",
    "            overall_segmentation = np.maximum(overall_segmentation, medsam_seg_resized)\n",
    "\n",
    "        # Calculate metrics directly\n",
    "        segmentation_mask = (overall_segmentation > 0).astype(np.uint8)\n",
    "\n",
    "        # Compute intersection and union of the masks\n",
    "        intersection = np.logical_and(ground_truth_gray > 0, segmentation_mask > 0)\n",
    "        union = np.logical_or(ground_truth_gray > 0, segmentation_mask > 0)\n",
    "\n",
    "        if np.sum(ground_truth_gray > 0) == 0 and np.sum(segmentation_mask > 0) == 0:\n",
    "            # Special case: both ground truth and segmentation are completely background\n",
    "            dice_score = 1.0\n",
    "            jaccard_score = 1.0\n",
    "            sensitivity = 1.0\n",
    "            precision = 1.0\n",
    "            accuracy = 1.0\n",
    "        else:\n",
    "            # Compute Dice score\n",
    "            dice_score = 2 * np.sum(intersection) / (np.sum(ground_truth_gray > 0) + np.sum(segmentation_mask > 0))\n",
    "\n",
    "            # Compute Jaccard index (IoU)\n",
    "            jaccard_score = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "            # Compute sensitivity (recall)\n",
    "            true_positives = np.sum(intersection)\n",
    "            false_negatives = np.sum((ground_truth_gray > 0) & (segmentation_mask == 0))\n",
    "            sensitivity = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "            # Compute precision\n",
    "            false_positives = np.sum((ground_truth_gray == 0) & (segmentation_mask > 0))\n",
    "            precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "            # Compute accuracy\n",
    "            true_negatives = np.sum((ground_truth_gray == 0) & (segmentation_mask == 0))\n",
    "            accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "\n",
    "        dice_scores.append(dice_score)\n",
    "        jaccard_scores.append(jaccard_score)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        precision_scores.append(precision)\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "        # Save some images for visualization\n",
    "        if idx < num_visualizations:\n",
    "            visualized_images.append((image, ground_truth_gray, segmentation_mask))\n",
    "\n",
    "    else:\n",
    "        # Process background-only images\n",
    "        with torch.no_grad():\n",
    "            image_embedding = medsam_model.image_encoder(img_1024_tensor) \n",
    "\n",
    "        # Full image segmentation for background-only images\n",
    "        medsam_seg = medsam_inference(medsam_model, image_embedding, None, H, W) \n",
    "\n",
    "        # Post-process to remove small spurious detections\n",
    "        _, medsam_seg_thresholded = cv2.threshold(medsam_seg, 1, 255, cv2.THRESH_BINARY)\n",
    "        medsam_seg_thresholded = medsam_seg_thresholded.astype(np.uint8)\n",
    "        num_labels, labels_im = cv2.connectedComponents(medsam_seg_thresholded)\n",
    "\n",
    "        # Only keep components larger than 50 pixels, meaning that it does not select background label pixels only, but the rest of masks are taken into account\n",
    "        for label in range(1, num_labels):\n",
    "            if np.sum(labels_im == label) < 50:\n",
    "                medsam_seg[labels_im == label] = 0\n",
    "\n",
    "        # Validate the output\n",
    "        assert np.sum(medsam_seg) == 0, \"The segmentation output for background-only images should be all zeros after post-processing.\"\n",
    "\n",
    "        # Case where both ground truth and segmentation are completely background\n",
    "        dice_scores.append(1.0)\n",
    "        jaccard_scores.append(1.0)\n",
    "        sensitivity_scores.append(1.0)\n",
    "        precision_scores.append(1.0)\n",
    "        accuracy_scores.append(1.0)\n",
    "\n",
    "        # Save some images for visualization\n",
    "        if idx < num_visualizations:\n",
    "            visualized_images.append((image, ground_truth_gray, medsam_seg))\n",
    "\n",
    "# Print average inference time\n",
    "average_inference_time = np.mean(inference_times)\n",
    "print(f\"Average Inference Time: {average_inference_time:.4f} seconds\")\n",
    "\n",
    "# Calculate throughput (images per second)\n",
    "total_images_processed = len(images)\n",
    "throughput = total_images_processed / np.sum(inference_times)\n",
    "print(f\"Throughput: {throughput:.2f} images per second\")\n",
    "\n",
    "# Calculate average scores\n",
    "average_dice_score = np.mean(dice_scores)\n",
    "average_jaccard_score = np.mean(jaccard_scores)\n",
    "average_sensitivity = np.mean(sensitivity_scores)\n",
    "average_precision = np.mean(precision_scores)\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "print(\"Average Dice Score:\", average_dice_score)\n",
    "print(\"Average Jaccard Index (IoU):\", average_jaccard_score)\n",
    "print(\"Average Sensitivity (Recall):\", average_sensitivity)\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "\n",
    "# Display some images with predictions for visual verification\n",
    "for i, (img, gt, pred) in enumerate(visualized_images):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Image {i+1}')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(gt, cmap='gray')\n",
    "    plt.title(f'Ground Truth {i+1}')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred, cmap='gray')\n",
    "    plt.title(f'Prediction {i+1}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Function to get model size\n",
    "def get_model_size(model):\n",
    "    # Save the state dictionary of the model\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    \n",
    "    # Get the size of the saved state dictionary file\n",
    "    model_size_MB = os.path.getsize(\"temp.pth\") / (1024 * 1024)  \n",
    "    \n",
    "    os.remove(\"temp.pth\")\n",
    "    \n",
    "    return model_size_MB\n",
    "\n",
    "model_size = get_model_size(medsam_model)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822f574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
